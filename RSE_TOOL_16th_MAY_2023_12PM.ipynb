{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMj7TqGADeinHWDYuKKU/7u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gaurav7004/RSE_TOOL/blob/main/RSE_TOOL_16th_MAY_2023_12PM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "y-KFBWnpcdJe"
      },
      "outputs": [],
      "source": [
        "# ! pip install pandarallel\n",
        "# from pandarallel import pandarallel\n",
        "import json\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pandarallel.initialize()"
      ],
      "metadata": {
        "id": "tsdvu58itpPJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Data1.csv')\n",
        "\n",
        "# remove blank spaces from front and back of column names\n",
        "df.columns = df.columns.str.strip()\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j_AWweohegUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fe2fd8-3241-4b5a-b6b6-6fdd9973b69b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fc8b3e05b5c4>:1: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('Data1.csv')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SEC', 'ST', 'FSU', 'IDNO', 'SEG', 'SSS', 'SSU', 'STRM', 'SSTRM', 'SS',\n",
              "       'NSS', 'NSC', 'MULT', 'HH_SIZE', 'HHTYPE', 'RELG', 'SG', 'MHCE',\n",
              "       'DC_ALL', 'DC_ST', 'SRL', 'REL', 'SEX', 'MSEX', 'AGE', 'MARST',\n",
              "       'GEDU_LVL', 'PAS', 'PS_SS', 'CWS', 'ERN_SELF', 'ERN_REG'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Data is the dictionary\n",
        "data = {}\n",
        "\n",
        "## Keys\n",
        "key = ''\n",
        "\n",
        "## values\n",
        "values = []\n",
        "\n",
        "## Open\n",
        "with open('Example1.RSE', \"r\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line.startswith('#'):\n",
        "            if key:\n",
        "                key = key.split(\"'\")[0]\n",
        "                key = key.split(\"\\t\")[0]\n",
        "                data[key] = [v for v in values if v and not v.startswith(\"'\")]\n",
        "            key = line.split(\"'\")[0]\n",
        "            key = key.split(\"\\t\")[0]\n",
        "            values = []\n",
        "        elif line.startswith(\"'\"):\n",
        "            pass\n",
        "        else:\n",
        "            line = line.split(\"'\")[0]\n",
        "            values.append(line.strip())\n",
        "\n",
        "# add last group\n",
        "if key:\n",
        "    data[key] = [v for v in values if v and not v.startswith(\"'\")]\n",
        "\n",
        "print(json.dumps(data, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCaBn0Ltw5Ac",
        "outputId": "1fa96f6a-e889-42dc-ea13-32b993e0c758"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"#SAMPLING SUBSAMPLE2\": [],\n",
            "    \"#GROUP 2\": [\n",
            "        \"SEC   C sec\",\n",
            "        \"ST_GR C NEWVAR\"\n",
            "    ],\n",
            "    \"#FILTER 1\": [\n",
            "        \"AGE>=15\"\n",
            "    ],\n",
            "    \"#VARIABLE 3  \": [\n",
            "        \"POP\\tN  NEWVAR\",\n",
            "        \"LF\\tN  NEWVAR\",\n",
            "        \"WRK\\tN  NEWVAR\"\n",
            "    ],\n",
            "    \"#FILE Data1.Xlsx\": [],\n",
            "    \"#RENAME  5    \": [\n",
            "        \"STRMID C sec+st+strm+sstrm\",\n",
            "        \"SS     C ss\",\n",
            "        \"NSS    N  nss\",\n",
            "        \"NSC    N  nsc\",\n",
            "        \"MULT   N  mult\"\n",
            "    ],\n",
            "    \"#TRANSFORM 4  \": [\n",
            "        \"ST_GR=(\\\"G1\\\")   ST in (\\\"10,19,20,21\\\")\",\n",
            "        \"ST_GR=(\\\"G2\\\")   ST in (\\\"11,12,13,14,15,16,17,18\\\")\",\n",
            "        \"ST_GR=(\\\"G3\\\")   Otherwise\",\n",
            "        \"POP=(1)\",\n",
            "        \"WRK=(1)  CWS in (\\\"11,12,21,31,41,51,61,61,71,72\\\")\",\n",
            "        \"WRK=(0)  CWS NOT in (\\\"11,12,21,31,41,51,61,61,71,72\\\")\",\n",
            "        \"LF=(1)   CWS in (\\\"11,12,21,31,41,51,61,61,71,72,81\\\")\",\n",
            "        \"LF=(0)   Otherwise\"\n",
            "    ],\n",
            "    \"#EST_RSE 7\": [\n",
            "        \"S POP\",\n",
            "        \"E POP\",\n",
            "        \"R POP\",\n",
            "        \"E 100*LF/POP\",\n",
            "        \"R LF/POP\",\n",
            "        \"E 100*WRK/POP\",\n",
            "        \"R WRK/POP\"\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def SAMPLING_(data, df):\n",
        "    for key, value in data.items():\n",
        "        ## To read the sampling method used\n",
        "        if str('SAMPLING').lower() in str(key).lower():\n",
        "            if str('SUBSAMPLE2').lower() in str(key).lower():\n",
        "                return str('SUBSAMPLE2')\n",
        "\n",
        "            elif str('SRSWR').lower() in str(key).lower():\n",
        "                pass\n",
        "            elif str('SRSWOR').lower() in str(key).lower():\n",
        "                pass\n",
        "\n",
        "SAMPLING_(data, df)"
      ],
      "metadata": {
        "id": "Ja3Sc1tvDbc_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "30465b65-185d-4110-bfb7-00060072f63c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SUBSAMPLE2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jZhrNagLdQz",
        "outputId": "c0c8acae-4159-4508-9071-1f1d3a4b8209"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['SEC', 'ST', 'FSU', 'IDNO', 'SEG', 'SSS', 'SSU', 'STRM', 'SSTRM', 'SS',\n",
              "       'NSS', 'NSC', 'MULT', 'HH_SIZE', 'HHTYPE', 'RELG', 'SG', 'MHCE',\n",
              "       'DC_ALL', 'DC_ST', 'SRL', 'REL', 'SEX', 'MSEX', 'AGE', 'MARST',\n",
              "       'GEDU_LVL', 'PAS', 'PS_SS', 'CWS', 'ERN_SELF', 'ERN_REG', 'AGE_GR',\n",
              "       'STRMID', 'POP', 'LF', 'WRK'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def GROUP_(data, df):\n",
        "    for key, value in data.items():    \n",
        "        if str('GROUP').lower() in str(key).lower():\n",
        "            value = [s.split()[0] for s in value]\n",
        "            for name in value:\n",
        "                if name in df.columns:\n",
        "                    pass\n",
        "                elif name not in df.columns:\n",
        "                    df[name] = 0\n",
        "\n",
        "            return value\n",
        "\n",
        "\n",
        "    for key, value in data.items():\n",
        "        ### To read the group columns\n",
        "        if str('GROUP').lower() in str(key).lower():\n",
        "            # Define regular expression to match three words separated by whitespace\n",
        "            regex = r'\\w+\\s+\\w+\\s+\\w+'\n",
        "\n",
        "            for i in range(len(value)):\n",
        "                # Find the first match in the string\n",
        "                match = re.search(regex, value[i])\n",
        "\n",
        "                # Print the match\n",
        "                lst = match.group().split()\n",
        "\n",
        "                if lst[0] in df.columns:\n",
        "                    if lst[1] == 'C':\n",
        "                        df[lst[0]] = df[lst[0]].astype(str)\n",
        "                elif lst[0] not in df.columns:\n",
        "                    # if lst[1] == 'C':\n",
        "                    #     df[lst[0]] = ''\n",
        "                    pass\n",
        "\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def FILTER_(data, df):\n",
        "    lst_queries = []\n",
        "    for key, value in data.items():\n",
        "\n",
        "        if str('FILTER').lower() in str(key).lower():\n",
        "            # Define regular expression to match three words separated by whitespace\n",
        "            regex = r'\\([^()]+\\)|\\w+[<>!=]+[0-9]+(?:\\s*(?:&&|\\|\\|)\\s*(?:\\([^()]+\\)|\\w+[<>!=]+[0-9]+))*'\n",
        "\n",
        "            for i in range(len(value)):\n",
        "                # Find the first match in the string\n",
        "                match = re.search(regex, value[i])\n",
        "\n",
        "                # Print the match\n",
        "                lst_queries.append(match.group())\n",
        "\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    filter_query = ' & '.join(['({})'.format(item) for item in lst_queries])\n",
        "    print(filter_query)\n",
        "    df = df.query(filter_query)\n",
        "    print(df)\n",
        "\n",
        "FILTER_(data, df)\n",
        "\n",
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def FILE_(data):\n",
        "    for key, value in data.items():\n",
        "        ### To read the file used\n",
        "        if str('FILE').lower() in str(key).lower():\n",
        "            if key.strip().endswith('.xlsx') or key.strip().endswith('.csv'):\n",
        "                match = re.search(r'[A-Za-z0-9]+\\.xlsx|[A-Za-z0-9]+\\.csv', key)   \n",
        "                if match:\n",
        "                    file_name = match.group(0)\n",
        "                    return file_name\n",
        "                else:\n",
        "                    file_name = None\n",
        "                    return file_name\n",
        "                print(file_name)\n",
        "\n",
        "            # return file_name\n",
        "\n",
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def RENAME_(data, df):\n",
        "    # create an empty dictionary to store the column names\n",
        "    column_dict = {}\n",
        "\n",
        "    for key, value in data.items():\n",
        "        ### To read the file used\n",
        "        if str('RENAME').lower() in str(key).lower():\n",
        "            # value = [elem for elem in value[0].split(' ') if elem.strip()]\n",
        "            # return value\n",
        "\n",
        "            for col in value:\n",
        "                col_split = col.split()\n",
        "\n",
        "                # print(col_split[0])\n",
        "\n",
        "                # extract the last element as the column name\n",
        "                col_name = col_split[-1]\n",
        "\n",
        "                new_col_name = col_split[0]\n",
        "\n",
        "                ### if there is a \"+\" sign in the last element, concatenate the columns\n",
        "                if \"+\" in col_name:\n",
        "                    col_concat = col_name.split(\"+\")\n",
        "                    col_concat = [i.upper() for i in col_concat]\n",
        "\n",
        "                    # concatenate the columns\n",
        "                    # df[new_col_name] = df.loc[:, col_concat].apply(lambda x: ''.join(['0' + str(i) if i < 10 else str(i) for i in x]), axis=1)\n",
        "                    # df[new_col_name] = df.loc[:, col_concat].apply(lambda x: ''.join(['0' + str(i) if i < 10 else str(i) for i in x]), axis=1)\n",
        "                    df[new_col_name] = df.loc[:, col_concat].apply(lambda x: ''.join(['0' + str(i) if int(i) < 10 else str(i) for i in x]), axis=1)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    col_name = col_name.upper()\n",
        "\n",
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def NEWVARIABLE_(data, df):\n",
        "    for key, value in data.items():\n",
        "        ### To read the file used\n",
        "        if str('VARIABLE').lower() in str(key).lower():\n",
        "            value = [s.split()[0] for s in value]\n",
        "            \n",
        "            for name in value:\n",
        "                df[name] = 1\n",
        "\n",
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def TRANSFORM_(data):\n",
        "    global condition\n",
        "\n",
        "    # Initialize empty arrays to store the values\n",
        "    Arr1 = []\n",
        "    Arr2 = []\n",
        "    Arr3 = []\n",
        "\n",
        "    for key, value in data.items():\n",
        "        ### To read the file used\n",
        "        if str('TRANSFORM').lower() in str(key).lower():\n",
        "            lines = '\\n'.join(value)\n",
        "            lines = lines.split(\"\\n\")\n",
        "\n",
        "            # Loop through the lines and extract the values\n",
        "            for line in lines:\n",
        "                line_values = line.split('=')  # split the line into values\n",
        "                print(line_values)\n",
        "                var_name = line_values[0].strip()  # extract the variable name\n",
        "                \n",
        "                var_value = line_values[1].strip().split()[0]  # extract the variable value\n",
        "\n",
        "                            \n",
        "                # Regular expression pattern to extract the condition\n",
        "                pattern = r'^\\w+=\\([^\\)]*\\)(?:\\s+(.*?)(?:\\n|$))?$'\n",
        "\n",
        "                # Split the string into lines and extract the condition from each line\n",
        "                conditions = []\n",
        "\n",
        "                for line in line.split('\\n'):\n",
        "                    match = re.match(pattern, line)\n",
        "                 \n",
        "                    if match:\n",
        "                        condition = match.group(1)\n",
        "                        \n",
        "                        if condition is not None:\n",
        "                            conditions.append(condition.strip())\n",
        "                        else:\n",
        "                            conditions.append(None)\n",
        "\n",
        "                # Append the values to the arrays\n",
        "                Arr1.append(var_name)\n",
        "                Arr2.append(var_value)\n",
        "\n",
        "                Arr3.append(condition)\n",
        "\n",
        "    return Arr1, Arr2, Arr3\n",
        "        \n",
        "### ---------------------------------------------------------------------------------------------------------\n",
        "def EST_RSE_(data):\n",
        "    for key, value in data.items():\n",
        "        pass\n",
        "\n",
        "GROUP_(data, df)\n",
        "TRANSFORM_(data)\n",
        "RENAME_(data, df)\n",
        "NEWVARIABLE_(data, df)\n",
        "FILE_(data)\n",
        "FILTER_(data, df)\n"
      ],
      "metadata": {
        "id": "JOeTfpj33fid",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "3972cce1-87e7-4fd7-a2bd-7af0345a495c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ST_GR', '(\"G1\")   ST in (\"10,19,20,21\")']\n",
            "['ST_GR', '(\"G2\")   ST in (\"11,12,13,14,15,16,17,18\")']\n",
            "['ST_GR', '(\"G3\")   Otherwise']\n",
            "['POP', '(1)']\n",
            "['WRK', '(1)  CWS in (\"11,12,21,31,41,51,61,61,71,72\")']\n",
            "['WRK', '(0)  CWS NOT in (\"11,12,21,31,41,51,61,61,71,72\")']\n",
            "['LF', '(1)   CWS in (\"11,12,21,31,41,51,61,61,71,72,81\")']\n",
            "['LF', '(0)   Otherwise']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'AGE>=15'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORM_(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLyl0OMB27jy",
        "outputId": "c3a8ba92-cd58-45ad-c631-6db76f009cce"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ST_GR', '(\"G1\")   ST in (\"10,19,20,21\")']\n",
            "['ST_GR', '(\"G2\")   ST in (\"11,12,13,14,15,16,17,18\")']\n",
            "['ST_GR', '(\"G3\")   Otherwise']\n",
            "['POP', '(1)']\n",
            "['WRK', '(1)  CWS in (\"11,12,21,31,41,51,61,61,71,72\")']\n",
            "['WRK', '(0)  CWS NOT in (\"11,12,21,31,41,51,61,61,71,72\")']\n",
            "['LF', '(1)   CWS in (\"11,12,21,31,41,51,61,61,71,72,81\")']\n",
            "['LF', '(0)   Otherwise']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['ST_GR', 'ST_GR', 'ST_GR', 'POP', 'WRK', 'WRK', 'LF', 'LF'],\n",
              " ['(\"G1\")', '(\"G2\")', '(\"G3\")', '(1)', '(1)', '(0)', '(1)', '(0)'],\n",
              " ['ST in (\"10,19,20,21\")',\n",
              "  'ST in (\"11,12,13,14,15,16,17,18\")',\n",
              "  'Otherwise',\n",
              "  None,\n",
              "  'CWS in (\"11,12,21,31,41,51,61,61,71,72\")',\n",
              "  'CWS NOT in (\"11,12,21,31,41,51,61,61,71,72\")',\n",
              "  'CWS in (\"11,12,21,31,41,51,61,61,71,72,81\")',\n",
              "  'Otherwise'])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EST_RSE_(data)"
      ],
      "metadata": {
        "id": "VQ4j3Mni9tOd"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_query_string(query_string):\n",
        "    # Replace curly quotes with straight quotes\n",
        "    query_string = query_string.replace(\"‚Äù\", \"\\\"\")\n",
        "    \n",
        "    if 'in' in query_string:\n",
        "        if ('NOT in' or 'not in') in query_string:\n",
        "            query_string = re.sub(r'(\\b\\w+\\b)\\s+NOT in\\s*\\(([^)]+)\\)', r'~\\1.isin([\\2])', query_string)\n",
        "            query_string = re.sub(r'(\\b\\w+\\b)\\s+in\\s*\\(([^)]+)\\)', r'\\1.isin([\\2])', query_string)\n",
        "        elif 'in' in query_string:\n",
        "            # Replace 'in' operators with 'isin' and add list brackets\n",
        "            query_string = re.sub(r'(\\b\\w+\\b)\\s+in\\s*\\(([^)]+)\\)', r'\\1.isin([\\2])', query_string)\n",
        "        else:\n",
        "            pass\n",
        "            \n",
        "    else:\n",
        "        query_string = re.sub(r'''(\\b\\w+\\b)(\\s*[<>=!]+\\s*)('[^']*'|\"[^\"]*\")|(\\b\\w+\\b)(\\s*[<>=!]+\\s*)(\"[^]*'|\"[^\"]*\")''', r'\\1\\2\\3', query_string)\n",
        "        query_string = re.sub(r'''(\\b\\w+\\b)(\\s*[<>=!]+\\s*)(\\d+)''', r'\\1\\2\\3', query_string)\n",
        "\n",
        "    # Replace double quotes with single quotes\n",
        "    query_string = query_string.replace(\"\\\"\", \"'\")\n",
        "    \n",
        "    # Replace 'AND' with '&', 'OR' with '|', and add parentheses\n",
        "    query_string = query_string.replace(\" AND \", \" & \")\\\n",
        "                        .replace(\" OR \", \" | \").\\\n",
        "                            replace(\"&&\", '&').\\\n",
        "                                replace('||', '|').\\\n",
        "                                    replace('<>', '!=')\n",
        "    \n",
        "    # Replace column names with df[column] syntax\n",
        "    query_string = re.sub(r'(\\b\\w+\\b)=', r'df[\"\\1\"]==', query_string)\n",
        "\n",
        "\n",
        "    # Define a function to replace variable names with df[\"<variable_name>\"]\n",
        "    def replace_var(match):\n",
        "        return f'df[\"{match.group(0)}\"]'\n",
        "\n",
        "    # Use regex to replace variable names with df[\"<variable_name>\"]\n",
        "    pattern = re.compile(r'''\\b\\w+\\b(?=\\s*[<>=!]=*\\s*[\\'\"\\d])''')\n",
        "    new_string = pattern.sub(replace_var, query_string)\n",
        "\n",
        "    query_string = re.sub(r\"'(\\d+)'|\\\"(\\d+)\\\"\", lambda match: str(int(match.group(1) or match.group(2))), query_string)\n",
        "    query_string = re.sub(r'\\[(\\d+,\\s*)+\\d+\\]', lambda match: '[' + match.group(0).replace(\"\\'\", \"'\") + ']', query_string)\n",
        "    \n",
        "    return query_string\n",
        "\n",
        "\n",
        "final_parsed_query = []\n",
        "\n",
        "for i in list(TRANSFORM_(data))[2]:\n",
        "    # print(i)\n",
        "    if i:\n",
        "        parsed_query = parse_query_string(i)\n",
        "        final_parsed_query.append(parsed_query)\n",
        "    else:\n",
        "        final_parsed_query.append(i)\n",
        "\n",
        "### Calling the final_parsed_query function\n",
        "### ---------------------------------------\n",
        "final_parsed_query"
      ],
      "metadata": {
        "id": "_lTGE07ya3aO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b8cf4f-3dff-47c9-89dc-c27ca19705cb"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ST_GR', '(\"G1\")   ST in (\"10,19,20,21\")']\n",
            "['ST_GR', '(\"G2\")   ST in (\"11,12,13,14,15,16,17,18\")']\n",
            "['ST_GR', '(\"G3\")   Otherwise']\n",
            "['POP', '(1)']\n",
            "['WRK', '(1)  CWS in (\"11,12,21,31,41,51,61,61,71,72\")']\n",
            "['WRK', '(0)  CWS NOT in (\"11,12,21,31,41,51,61,61,71,72\")']\n",
            "['LF', '(1)   CWS in (\"11,12,21,31,41,51,61,61,71,72,81\")']\n",
            "['LF', '(0)   Otherwise']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"ST.isin(['10,19,20,21'])\",\n",
              " \"ST.isin(['11,12,13,14,15,16,17,18'])\",\n",
              " 'Otherwise',\n",
              " None,\n",
              " \"CWS.isin(['11,12,21,31,41,51,61,61,71,72'])\",\n",
              " \"~CWS.isin(['11,12,21,31,41,51,61,61,71,72'])\",\n",
              " \"CWS.isin(['11,12,21,31,41,51,61,61,71,72,81'])\",\n",
              " 'Otherwise']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['AGE'].unique()"
      ],
      "metadata": {
        "id": "GVTZkgr9YtIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67f7bbdd-73cf-4fc9-8969-2481d04ecbbe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 40.,  18.,  19.,  38.,  21.,  28.,   1.,  52.,  26.,  45.,  25.,\n",
              "        36.,  17.,  30.,  55.,  44.,  16.,  20.,  68.,  35.,  10.,  42.,\n",
              "        14.,  39.,  65.,  43.,  15.,  60.,  48.,  22.,   8.,   9.,  13.,\n",
              "        27.,  24.,  62.,  23.,   0.,  56.,   2.,   4.,  66.,  47.,  50.,\n",
              "        57.,  61.,  58.,  31.,   7.,  34.,  64.,   6.,  54.,  11.,  29.,\n",
              "        32.,  41.,  59.,  51.,  75.,  70.,  12.,  37.,  33.,  85.,  72.,\n",
              "         3.,   5.,  53.,  73.,  46.,  67.,  49.,  80.,  63.,  78.,  71.,\n",
              "        82.,  81.,  69.,  74.,  86.,  76.,  84.,  83.,  79.,  95.,  87.,\n",
              "        90.,  77.,  89., 110.,  96., 100.,  88.,  93.,  92.,  98.,  94.,\n",
              "       105.,  91.,  99.,  97., 109.])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Arr1 = list(TRANSFORM_(data))[0]\n",
        "Arr2 = list(TRANSFORM_(data))[1]\n",
        "\n",
        "# print(Arr2)\n",
        "Arr2 = [i.strip('()\\'\\\"') for i in Arr2]\n",
        "Arr3 = final_parsed_query\n",
        "\n",
        "Arr2 = [float(elem) if elem.replace('.', '', 1).isdigit() else elem for elem in Arr2]\n",
        "print(Arr2)\n",
        "\n",
        "# print(Arr1)\n",
        "# print(Arr2)\n",
        "# print(Arr3)"
      ],
      "metadata": {
        "id": "NtMiw_fo7hvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "hYYh_PXq7C_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set(Arr1) - set(list(df.columns))"
      ],
      "metadata": {
        "id": "FpCHnWE634Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check for name mismatches and create new columns\n",
        "mismatched_names = set(Arr1) - set(df.columns)\n",
        "\n",
        "new_columns = {name: [0] * df.shape[0] for name in mismatched_names}\n",
        "df = df.assign(**new_columns)\n",
        "\n",
        "mismatched_names"
      ],
      "metadata": {
        "id": "2Jb5zGh67zYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr3"
      ],
      "metadata": {
        "id": "SQYkxNyITODY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr3 = [i.replace(\"(['\", \"([\").replace(\"'])\", \"])\") if i is not None else 'True' if Arr3[j] == 'Otherwise' else True for j, i in enumerate(Arr3)]\n",
        "Arr3"
      ],
      "metadata": {
        "id": "YlL6LYR-43rC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_arr3 = []\n",
        "for i, value in enumerate(Arr3):\n",
        "    if value == 'Otherwise':\n",
        "        other_values = set(['~(' + Arr3[j] + ')' for j in range(len(Arr3)) if Arr1[j] == Arr1[i] and Arr3[j] != 'Otherwise'])\n",
        "        new_value = ' & '.join(other_values)\n",
        "        print(new_value)\n",
        "        new_arr3.append(new_value)\n",
        "    else:\n",
        "        new_arr3.append(value)\n",
        "\n",
        "# new_arr3"
      ],
      "metadata": {
        "id": "BWIYIIl_5_ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr3 = new_arr3"
      ],
      "metadata": {
        "id": "WNp2AGBEaJl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "M7YnPjuAAY53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr2"
      ],
      "metadata": {
        "id": "BiNMunJ16H3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(Arr2)):\n",
        "    if type(Arr2[i]) == str:\n",
        "        print(Arr2[i])"
      ],
      "metadata": {
        "id": "CXIrpfUnv-cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0VEa4lv267ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the queries in Arr3 to filter the rows of the dataframe\n",
        "for i, query_str in enumerate(Arr3):\n",
        "    \n",
        "    if query_str is True:\n",
        "        if type(Arr2[i]) == str:\n",
        "            if '+' in Arr2[i]:\n",
        "                df[Arr1[i]] = df[Arr2[i].split('+')].apply(lambda x: x.sum(), axis=1)\n",
        "                print('*** First if', Arr2[i], Arr1[i], Arr3[i], i)\n",
        "\n",
        "            else:\n",
        "                df[Arr1[i]] = df[[Arr2[i]]].apply(lambda x: x, axis=1)\n",
        "                print('*** First else', Arr2[i], Arr1[i], Arr3[i], i)\n",
        "        # if type(Arr3[i]):\n",
        "        #     print('^^^ Second if', Arr2[i], Arr1[i], Arr3[i], i)\n",
        "        #     df.loc[df_query.index, Arr1[i]] = Arr2[i]\n",
        "    \n",
        "    else:\n",
        "        try:\n",
        "            df_query = df.query(query_str)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        if type(Arr2[i]) == float:\n",
        "            df.loc[df_query.index, Arr1[i]] = Arr2[i]\n",
        "            # print('### Third if', Arr2[i], Arr1[i], Arr3[i])\n",
        "        else:\n",
        "            df.loc[df_query.index, Arr1[i]] = Arr2[i]\n",
        "            # print('@@@ First else', Arr2[i], Arr1[i], Arr3[i])\n",
        "\n",
        "\n",
        "        if type(Arr2[i]) == str:\n",
        "            if Arr2[i] in list(df.columns):\n",
        "                df[Arr1[i]] = df[[Arr2[i]]].apply(lambda x: x, axis=1)\n",
        "                # print('$$$ Fourth if', Arr2[i], Arr1[i], Arr3[i])\n",
        "\n",
        "# print(df)"
      ],
      "metadata": {
        "id": "9Ymd7BTvha86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['POP']].value_counts()"
      ],
      "metadata": {
        "id": "I_UBlt_QSS0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "ivucwi5eyIuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mckCl-hXyex3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('WorkFile_Balanced_Half_Sample_Method_Example1.csv')"
      ],
      "metadata": {
        "id": "4wqjFwT1DjuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Iuk_3yl_Nu5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Example dataframe\n",
        "# df = pd.DataFrame({\n",
        "#     'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n",
        "#     'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n",
        "#     'C': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "#     'D': [10, 20, 30, 40, 50, 60, 70, 80],\n",
        "#     'E': ['foo1', 'bar1', 'foo1', 'bar1', 'foo1', 'bar1', 'foo1', 'foo1'],\n",
        "#     'F': ['one2', 'one2', 'two2', 'three2', 'two2', 'two2', 'one2', 'three2'],\n",
        "#     'G': [101, 201, 301, 401, 501, 601, 701, 801],\n",
        "# })\n",
        "\n",
        "# # Define the prefix to use for the new columns\n",
        "# prefix = 'Z'\n",
        "\n",
        "# lst = ['T1', 'T2', 'T3']\n",
        "\n",
        "# for idx, col in enumerate(df.columns):\n",
        "#     for i in range(len(lst)):\n",
        "#         # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "#         new_col_val = prefix * (i+1)\n",
        "#         # Add the new column to the DataFrame\n",
        "#         df[lst[i]] = new_col_val\n",
        "\n",
        "\n",
        "# # Print the resulting DataFrame\n",
        "# print(df)"
      ],
      "metadata": {
        "id": "V8r6iLdbkVoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "iqbErrIfg852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import itertools\n",
        "\n",
        "# # Example dataframe\n",
        "# df = pd.DataFrame({\n",
        "#     'A': ['foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'foo'],\n",
        "#     'B': ['one', 'one', 'two', 'three', 'two', 'two', 'one', 'three'],\n",
        "#     'C': [1, 2, 3, 4, 5, 6, 7, 8],\n",
        "#     'D': [10, 20, 30, 40, 50, 60, 70, 80],\n",
        "#     'E': ['foo1', 'bar1', 'foo1', 'bar1', 'foo1', 'bar1', 'foo1', 'foo1'],\n",
        "#     'F': ['one2', 'one2', 'two2', 'three2', 'two2', 'two2', 'one2', 'three2'],\n",
        "#     'G': [101, 201, 301, 401, 501, 601, 701, 801],\n",
        "# })\n",
        "\n",
        "# lst_df = list(df.columns)\n",
        "\n",
        "# # Select columns to group by\n",
        "# groupby_cols = ['A', 'B', 'E', 'C']\n",
        "\n",
        "# # Create a list of all possible subsets of groupby_cols\n",
        "# groupby_cols_subsets = [list(subset) for i in range(len(groupby_cols) + 1)\n",
        "#                         for subset in itertools.combinations(groupby_cols, i)]\n",
        "\n",
        "# print(groupby_cols_subsets)\n",
        "\n",
        "# # Select columns that are not in groupby_cols\n",
        "# cols = [col for col in df.columns if col not in groupby_cols]\n",
        "\n",
        "# agg_dict = {col: sum for col in cols if df[col].dtype == int or df[col].dtype == float}\n",
        "\n",
        "# i = 0\n",
        "# # Loop over each subset of columns and perform the aggregation\n",
        "# for groupby_cols_subset in groupby_cols_subsets:\n",
        "#     # Define the prefix to use for the new columns\n",
        "#     prefix = 'Z'\n",
        "\n",
        "#     if len(groupby_cols_subset) == 0:\n",
        "\n",
        "#         # # If empty list is selected, perform aggregation without grouping\n",
        "#         new_df = df.agg(agg_dict).to_frame().T\n",
        "\n",
        "#         test_lst = list(set(lst_df) - set(list(new_df.columns)))\n",
        "\n",
        "#         new_df['groupby_cols'] = 'All columns'\n",
        "\n",
        "#         new_col_val_ = []\n",
        "#         for i in range(len(test_lst)):\n",
        "#             # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "#             new_col_val_.append(prefix * (i+1))\n",
        "\n",
        "#         for i in range(len(new_col_val_)):\n",
        "#             # Add the new column to the DataFrame\n",
        "#             new_df[test_lst[i]] = new_col_val_[i]\n",
        "        \n",
        "#         i += 1\n",
        "#         print(new_df, '\\n')\n",
        "\n",
        "#         continue\n",
        "\n",
        "#     # Create a new dataframe by grouping by the selected columns and aggregating\n",
        "#     # Note: here I'm just calculating the sum of columns C and D, but you can use any aggregation function\n",
        "#     new_df = df.groupby(list(groupby_cols_subset)).agg(agg_dict).reset_index()\n",
        "\n",
        "#     test_lst = list(set(lst_df) - set(list(new_df.columns)))\n",
        "\n",
        "#     # # Add a new column to indicate which groupby columns are selected\n",
        "#     new_df['groupby_cols'] = ', '.join(groupby_cols_subset)\n",
        "\n",
        "#     new_col_val = []\n",
        "#     for i in range(len(test_lst)):\n",
        "#         # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "#         new_col_val.append(prefix * (i+1))\n",
        "\n",
        "#     for i in range(len(new_col_val)):\n",
        "#         # Add the new column to the DataFrame\n",
        "#         new_df[test_lst[i]] = new_col_val[i]\n",
        "\n",
        "#     i += 1\n",
        "    \n",
        "#     print(new_df, '\\n')\n"
      ],
      "metadata": {
        "id": "iyj9g6pDPbbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "UezoawpS4vn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['SEC'].unique()"
      ],
      "metadata": {
        "id": "zXY5TUr4rJsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PW7_Ab1ETtzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 1**"
      ],
      "metadata": {
        "id": "OS08Y3aCseyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Read the data from the CSV file\n",
        "# df = pd.read_csv('Work_File1A.csv')\n",
        "\n",
        "# Get the current column names\n",
        "old_col_names = df.columns.tolist()\n",
        "\n",
        "# Create a dictionary with the new column names\n",
        "new_col_names = {col: col.strip() for col in old_col_names}\n",
        "\n",
        "# Rename the columns using the dictionary\n",
        "df.rename(columns=new_col_names, inplace=True)\n",
        "\n",
        "# Calculate the SS_MULT and CMULT fields\n",
        "df['SS_MULT'] = df['MULT'] / 100\n",
        "df['CMULT'] = df.apply(lambda row: row['MULT'] / 200 if row['NSC'] > row['NSS'] else row['MULT'] / 100, axis=1)\n",
        "\n",
        "lst_df = list(df.columns)\n",
        "\n",
        "groupby_cols = GROUP_(data, df)\n",
        "\n",
        "# Create a list of all possible subsets of groupby_cols\n",
        "groupby_cols_subsets = [list(subset) for i in range(len(groupby_cols) + 1)\n",
        "                        for subset in itertools.combinations(groupby_cols, i)]\n",
        "\n",
        "# Select columns that are not in groupby_cols\n",
        "cols = [col for col in df.columns if col in groupby_cols]\n",
        "\n",
        "agg_dict = {col: sum for col in cols if (df[col].dtype == int or df[col].dtype == float) and col in groupby_cols}\n",
        "\n",
        "################\n",
        "Arr_W1 = []\n",
        "\n",
        "\n",
        "### Loop over each subset of columns and perform the aggregation\n",
        "### ------------------------------------------------------------\n",
        "for groupby_cols_subset in groupby_cols_subsets:\n",
        "    # Define the prefix to use for the new columns\n",
        "    prefix = 'Z'\n",
        "\n",
        "    if len(groupby_cols_subset) == 0:\n",
        "\n",
        "        # # If empty list is selected, perform aggregation without grouping\n",
        "        # w1 = df.agg(agg_dict).to_frame().T\n",
        "\n",
        "        # print(new_df)\n",
        "\n",
        "        test_lst = groupby_cols\n",
        "\n",
        "        df['POP'] = pd.to_numeric(df['POP']) * df['CMULT']\n",
        "        df['LF'] = pd.to_numeric(df['LF']) * df['CMULT']\n",
        "        df['WRK'] = pd.to_numeric(df['WRK']) * df['CMULT']\n",
        "        df['MULT'] = df.shape[0]\n",
        "\n",
        "        agg_dict = {'POP': 'sum', 'LF': 'sum', 'WRK': 'sum'}\n",
        "\n",
        "        # Group the data by SEC and ST_GR and calculate the aggregates\n",
        "        w1 = df.agg(agg_dict).to_frame().T\n",
        "\n",
        "        # Rename the columns and reset the index\n",
        "        w1 = w1.rename(columns={'POP': 'pophat', 'LF': 'lfhat', 'WRK': 'wrkhat', 'MULT': 'count'}).reset_index()\n",
        "        w1['groupby_cols'] = 'All columns'\n",
        "\n",
        "        new_col_val_ = []\n",
        "\n",
        "        for i in range(len(list(set(groupby_cols_subsets[-1]) - set(w1.columns)))):\n",
        "            # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "            new_col_val_.append(prefix * (i+1))\n",
        "\n",
        "        for i in range(len(list(set(groupby_cols_subsets[-1]) - set(w1.columns)))):\n",
        "            # Add the new column to the DataFrame\n",
        "            w1[test_lst[i]] = new_col_val_[i]\n",
        "\n",
        "        # print(w1)\n",
        "        ################\n",
        "        Arr_W1.append(w1)\n",
        "\n",
        "        continue\n",
        "\n",
        "    # Create a new dataframe by grouping by the selected columns and aggregating\n",
        "    # Note: here I'm just calculating the sum of columns C and D, but you can use any aggregation function\n",
        "    # new_df = df.groupby(list(groupby_cols_subset)).agg(agg_dict).reset_index()\n",
        "    \n",
        "    # Group the data by SEC and ST_GR and calculate the aggregates\n",
        "    w1 = df.groupby(groupby_cols_subset).agg({'POP': lambda x: (pd.to_numeric(x) * df['CMULT']).sum(),\n",
        "                                                'LF': lambda x: (pd.to_numeric(x) * df['CMULT']).sum(),\n",
        "                                                    'WRK': lambda x: (pd.to_numeric(x) * df['CMULT']).sum(),\n",
        "                                                        'MULT': 'count'}).reset_index()\n",
        "\n",
        "    # Rename the columns and reset the index\n",
        "    w1 = w1.rename(columns={'POP': 'pophat', 'LF': 'lfhat', 'WRK': 'wrkhat', 'MULT': 'no_sam'}).reset_index()\n",
        "\n",
        "    test_lst = groupby_cols_subset\n",
        "\n",
        "    # Add a new column to indicate which groupby columns are selected\n",
        "    w1['groupby_cols'] = ', '.join(groupby_cols_subset)\n",
        "\n",
        "    new_col_val = []\n",
        "    for i in range(len(list(set(groupby_cols_subsets[-1]) - set(w1.columns)))):\n",
        "        # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "        new_col_val.append(prefix * (i+1))\n",
        "\n",
        "    for i in range(len(list(set(groupby_cols_subsets[-1]) - set(w1.columns)))):\n",
        "        # Add the new column to the DataFrame\n",
        "        w1[list(set(groupby_cols_subsets[-1]) - set(w1.columns))[i]] = new_col_val[i]\n",
        "\n",
        "    Arr_W1.append(w1)\n"
      ],
      "metadata": {
        "id": "FBjN92PqdCZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W1"
      ],
      "metadata": {
        "id": "7OcfQmmShu-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['SS'].unique()"
      ],
      "metadata": {
        "id": "PeSjbY4gjGgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['no_sam'] = 0"
      ],
      "metadata": {
        "id": "XwIcaR1d0Tob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "2LV13Nph0Tr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 2** IT is running slow, figure it out later...\n"
      ],
      "metadata": {
        "id": "8flHj01dsJeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter dataframe to include only rows where SS is 1 or 2\n",
        "df = df.query(\"SS in [1, 2]\")\n",
        "# print(f\"Number of rows after filtering: {len(df)}\")\n",
        "\n",
        "Arr_W2 = []\n",
        "\n",
        "### Loop over subset of columns to perform aggregation\n",
        "for groupby_cols_subset in groupby_cols_subsets:\n",
        "\n",
        "    new_groupby_cols_subset = ['STRMID']\n",
        "\n",
        "    # Define the prefix to use for the new columns\n",
        "    prefix = 'Z'\n",
        "\n",
        "    if len(groupby_cols_subset) == 0:\n",
        "\n",
        "        df['popstr'] = pd.to_numeric(df['POP']) * df['CMULT']\n",
        "        df['s1pop'] = np.where(df['SS'] == 1, pd.to_numeric(df['POP']) * df['SS_MULT'], 0)\n",
        "        df['s2pop'] = np.where(df['SS'] == 2, pd.to_numeric(df['POP']) * df['SS_MULT'], 0)\n",
        "        df['lfstr'] = pd.to_numeric(df['LF']) * df['CMULT']\n",
        "        df['s1lf'] = np.where(df['SS'] == 1, pd.to_numeric(df['LF']) * df['SS_MULT'], 0)\n",
        "        df['s2lf'] = np.where(df['SS'] == 2, pd.to_numeric(df['LF']) * df['SS_MULT'], 0)\n",
        "        df['wrkstr'] = pd.to_numeric(df['WRK']) * df['CMULT']\n",
        "        df['s1wrk'] = np.where(df['SS'] == 1, pd.to_numeric(df['WRK']) * df['SS_MULT'], 0)\n",
        "        df['s2wrk'] = np.where(df['SS'] == 2, pd.to_numeric(df['WRK']) * df['SS_MULT'], 0)\n",
        "\n",
        "        agg_dict = {'popstr': 'sum', 's1pop': 'sum', 's2pop': 'sum', 'lfstr': 'sum', 's1lf': 'sum', \\\n",
        "                    's2lf': 'sum', 'wrkstr': 'sum', 's1wrk': 'sum', 's2wrk': 'sum'}\n",
        "\n",
        "        # # If empty list is selected, perform aggregation without grouping\n",
        "        w2 = df.agg(agg_dict).to_frame().T\n",
        "\n",
        "        w2['STRMID'] = 'ALL'\n",
        "\n",
        "        new_col_val_ = []\n",
        "\n",
        "        for i in range(len(list(set(groupby_cols) - set(w2.columns)))):\n",
        "            # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "            new_col_val_.append(prefix * (i+1))\n",
        "\n",
        "        for i in range(len(list(set(groupby_cols) - set(w2.columns)))):\n",
        "            # Add the new column to the DataFrame\n",
        "            w2[groupby_cols[i]] = new_col_val_[i]\n",
        "\n",
        "        print(w2)\n",
        "        Arr_W2.append(w2)\n",
        "\n",
        "        continue\n",
        "\n",
        "    new_groupby_cols_subset.append(groupby_cols_subset[0])\n",
        "\n",
        "    print(new_groupby_cols_subset)\n",
        "\n",
        "    w2 = df.groupby(new_groupby_cols_subset).agg(\n",
        "    popstr=('POP', lambda x: np.sum(x * df['CMULT'])),\n",
        "    s1pop=('POP', lambda x: np.nansum(np.where(df['SS'] == 1, x * df['SS_MULT'], 0))),\n",
        "    s2pop=('POP', lambda x: np.nansum(np.where(df['SS'] == 2, x * df['SS_MULT'], 0))),\n",
        "    lfstr=('LF', lambda x: np.nansum(x * df['CMULT'])),\n",
        "    s1lf=('LF', lambda x: np.nansum(np.where(df['SS'] == 1, x * df['SS_MULT'], 0))),\n",
        "    s2lf=('LF', lambda x: np.nansum(np.where(df['SS'] == 2, x * df['SS_MULT'], 0))),\n",
        "    wrkstr=('WRK', lambda x: np.nansum(x * df['CMULT'])),\n",
        "    s1wrk=('WRK', lambda x: np.nansum(np.where(df['SS'] == 1, x * df['SS_MULT'], 0))),\n",
        "    s2wrk=('WRK', lambda x: np.nansum(np.where(df['SS'] == 2, x * df['SS_MULT'], 0)))\n",
        "    ).reset_index()\n",
        "\n",
        "    new_col_val = []\n",
        "    for i in range(len(list(set(groupby_cols_subsets[-1]) - set(w2.columns)))):\n",
        "        # Create a new column with the appropriate number of \"Z\"s and the prefix\n",
        "        new_col_val.append(prefix * (i+1))\n",
        "\n",
        "    for i in range(len(list(set(groupby_cols_subsets[-1]) - set(w2.columns)))):\n",
        "        # Add the new column to the DataFrame\n",
        "        w2[list(set(groupby_cols_subsets[-1]) - set(w2.columns))[i]] = new_col_val[i]\n",
        "\n",
        "    Arr_W2.append(w2)"
      ],
      "metadata": {
        "id": "KN_zT5h1i4eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W2"
      ],
      "metadata": {
        "id": "rEtzMLU31Klt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W3 = []\n",
        "\n",
        "### Loop over subset of columns to perform aggregation\n",
        "for i in range(len(Arr_W2)):\n",
        "\n",
        "    # Join w2 and w1 on SEC and ST_GR columns\n",
        "    # w3 = Arr_W2[i].merge(Arr_W1[i], on=['SEC', 'AGE_GR'])\n",
        "    w3 = pd.concat([Arr_W2[i], Arr_W1[i]], axis=1)\n",
        "\n",
        "    w3['no_sam'] = w3.shape[0]\n",
        "    \n",
        "    # Add new columns to w3\n",
        "    w3['r1hat'] = w3['lfhat'] / w3['pophat']\n",
        "    w3['r2hat'] = w3['wrkhat'] / w3['pophat']\n",
        "\n",
        "    # Equate s1pop and s2pop\n",
        "    w3.loc[w3['popstr'] == w3['s1pop'], 's1pop'] = w3.loc[w3['popstr'] == w3['s1pop'], 's2pop']\n",
        "    w3.loc[w3['popstr'] == w3['s2pop'], 's2pop'] = w3.loc[w3['popstr'] == w3['s2pop'], 's1pop']\n",
        "\n",
        "    # Equate s1lf and s2lf\n",
        "    w3.loc[w3['lfstr'] == w3['s1lf'], 's1lf'] = w3.loc[w3['lfstr'] == w3['s1lf'], 's2lf']\n",
        "    w3.loc[w3['lfstr'] == w3['s2lf'], 's2lf'] = w3.loc[w3['lfstr'] == w3['s2lf'], 's1lf']\n",
        "\n",
        "    # Equate s1wrk and s2wrk\n",
        "    w3.loc[w3['wrkstr'] == w3['s1wrk'], 's1wrk'] = w3.loc[w3['wrkstr'] == w3['s1wrk'], 's2wrk']\n",
        "    w3.loc[w3['wrkstr'] == w3['s2wrk'], 's2wrk'] = w3.loc[w3['wrkstr'] == w3['s2wrk'], 's1wrk']\n",
        "    \n",
        "    Arr_W3.append(w3)\n",
        "\n"
      ],
      "metadata": {
        "id": "aOozfIhLjET1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W3[3].columns"
      ],
      "metadata": {
        "id": "Myt7II06w6Gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Drop duplicate columns\n",
        "for i in range(len(Arr_W3)):\n",
        "\n",
        "    Arr_W3[i] = Arr_W3[i].loc[:, ~Arr_W3[i].columns.duplicated()]\n",
        "\n",
        "    print(Arr_W3[i].columns)"
      ],
      "metadata": {
        "id": "l5dLy9MT5nOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W4 = []\n",
        "\n",
        "### Loop over subset of columns to perform aggregation\n",
        "for i in range(len(Arr_W3)):\n",
        "    mask = Arr_W3[i].columns.duplicated()\n",
        "    Arr_W3[i] = Arr_W3[i].loc[:, ~mask]\n",
        "\n",
        "    gby_list = ['no_sam', 'pophat', 'lfhat', 'wrkhat', 'r1hat', 'r2hat'] + groupby_cols\n",
        "\n",
        "    print(gby_list)\n",
        "\n",
        "    w4 = Arr_W3[i].groupby(gby_list).agg(\n",
        "\n",
        "        var_pop = ('s1pop', lambda x: np.sum((x - Arr_W3[i]['s2pop']) ** 2)),\n",
        "\n",
        "            mse_R1 = ('s1lf', lambda x: np.nansum(\n",
        "                        ((x - Arr_W3[i]['s2lf']) ** 2) +\n",
        "                            ((Arr_W3[i]['r1hat'] ** 2) * ((Arr_W3[i]['s1pop'] - Arr_W3[i]['s2pop']) ** 2) -\n",
        "                                (2 * Arr_W3[i]['r1hat'] * (x - Arr_W3[i]['s2lf']) * (Arr_W3[i]['s1pop'] - Arr_W3[i]['s2pop'])))\n",
        "                    )),\n",
        "\n",
        "                mse_R2 = ('s1wrk', lambda x: np.nansum(\n",
        "                            ((x - Arr_W3[i]['s2wrk']) ** 2) +\n",
        "                                ((Arr_W3[i]['r2hat'] ** 2) * ((Arr_W3[i]['s1pop'] - Arr_W3[i]['s2pop']) ** 2) -\n",
        "                                    (2 * Arr_W3[i]['r2hat'] * (x - Arr_W3[i]['s2wrk']) * (Arr_W3[i]['s1pop'] - Arr_W3[i]['s2pop'])))\n",
        "                        ),\n",
        "        \n",
        "    )).reset_index()\n",
        "\n",
        "    Arr_W4.append(w4)\n",
        "\n",
        "    print(Arr_W4[i])"
      ],
      "metadata": {
        "id": "qdsNdyRURiFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W5 = []\n",
        "for i in range(len(Arr_W4)):\n",
        "    # Calculate RSEs for each variable in T1\n",
        "    w5 = Arr_W4[i].assign(\n",
        "        RSE_POP=lambda x: 100 * (x['var_pop'] ** 0.5) / (2 * x['pophat']),\n",
        "        RSE_R1=lambda x: 100 * (x['mse_R1'] ** 0.5) / (2 * x['lfhat']),\n",
        "        RSE_R2=lambda x: 100 * (x['mse_R2'] ** 0.5) / (2 * x['wrkhat'])\n",
        "    )\n",
        "\n",
        "    Arr_W5.append(w5)"
      ],
      "metadata": {
        "id": "DrX5BiwtRiI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Arr_W5"
      ],
      "metadata": {
        "id": "UU3TQEp2UPPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tables = []\n",
        "\n",
        "for i in range(len(Arr_W5)):\n",
        "    gby_list = ['no_sam', 'pophat', 'lfhat', 'wrkhat', 'r1hat', 'r2hat', 'RSE_POP', 'RSE_R1', 'RSE_R2'] + groupby_cols\n",
        "    # assuming the data is loaded into a DataFrame called w5\n",
        "    T1 = Arr_W5[i][gby_list].copy()\n",
        "\n",
        "    # add empty columns for sec_desc and st_gr_desc\n",
        "    T1['sec_desc'] = ''\n",
        "    T1['st_gr_desc'] = ''\n",
        "\n",
        "    # apply formatting to columns\n",
        "    T1['R1'] = 100 * T1['r1hat']\n",
        "    T1['R2'] = 100 * T1['r2hat']\n",
        "\n",
        "    gby_list = ['sec_desc', 'st_gr_desc', 'no_sam', 'pophat','lfhat', 'wrkhat', 'RSE_POP', 'R1', 'RSE_R1', 'R2', 'RSE_R2'] + groupby_cols\n",
        "\n",
        "    # reorder columns\n",
        "    T1 = T1[gby_list]\n",
        "\n",
        "    gby_list = ['no_sam', 'pophat','lfhat', 'wrkhat', 'r1hat', 'r2hat', 'RSE_POP', 'RSE_R1', 'RSE_R2'] + groupby_cols\n",
        "\n",
        "    T1 = Arr_W5[i][gby_list].copy()\n",
        "\n",
        "    T1['sec_desc'] = ' '\n",
        "    T1['st_gr_desc'] = ' '\n",
        "\n",
        "    T1['R1'] = 100 * T1['r1hat']\n",
        "    T1['R2'] = 100 * T1['r2hat']\n",
        "\n",
        "    gby_list = ['no_sam', 'pophat','lfhat', 'wrkhat', 'RSE_POP', 'R1', 'RSE_R1', 'R2', 'RSE_R2'] + groupby_cols\n",
        "\n",
        "    T1 = T1[gby_list]\n",
        "\n",
        "    Tables.append(T1)\n",
        "    \n",
        "print(Tables)"
      ],
      "metadata": {
        "id": "bgkq9FL3RsR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Tables[1]"
      ],
      "metadata": {
        "id": "EFtW9GLWt5Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_dataframes(tables):\n",
        "    merged_df = tables[0]\n",
        "    for df in tables[1:]:\n",
        "        merged_df = pd.concat([merged_df, df], axis=0)\n",
        "    return merged_df\n",
        "\n",
        "merge_dataframes(Tables)"
      ],
      "metadata": {
        "id": "lsHDF6ulRubm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iz0cmdNY2MNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# T1.to_csv('T1_Balanced_Half_Sample_Method.csv')"
      ],
      "metadata": {
        "id": "Lus9B0NNRxUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "T1.columns"
      ],
      "metadata": {
        "id": "TL1edIn-Sbca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AlNWzzKTSbhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U9X030fuWb88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6hiFyTQWd2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}